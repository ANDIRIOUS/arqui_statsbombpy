{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del Modelo Live Win Probability (LWP)\n",
    "\n",
    "**Objetivo:** Entrenar un modelo de Machine Learning que prediga la probabilidad de victoria en vivo basado en el estado actual del partido.\n",
    "\n",
    "**Datos:** 4 temporadas de La Liga con datos 360 de StatsBomb\n",
    "\n",
    "**Salidas:** \n",
    "- `P(Victoria Local)`\n",
    "- `P(Empate)`\n",
    "- `P(Victoria Visitante)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete - 2025-11-06 04:42:25.332770\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import statsbombpy.sb as sb\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "print(f\"Setup complete - {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inicializar Spark Session\n",
    "\n",
    "**Nota:** Este notebook usa configuración CPU para entrenamiento batch. La configuración GPU/CPU se controla en `spark-conf/spark-defaults.conf` y actualmente está en **modo CPU** para evitar conflictos de GPU entre múltiples executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e4f05839-668d-45a5-b39d-2abbb492bfc1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 271ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e4f05839-668d-45a5-b39d-2abbb492bfc1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/7ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/06 04:42:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.3.1\n",
      "Spark Master: spark://spark-master:7077\n",
      "Spark UI: http://localhost:4040\n",
      "\n",
      "Spark Configuration:\n",
      "  RAPIDS enabled: false\n",
      "  GPU resources: none\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "# Note: GPU/CPU configuration is controlled by spark-defaults.conf\n",
    "# For training: CPU mode (GPU disabled in spark-defaults.conf)\n",
    "# For streaming: GPU mode (GPU enabled in spark-defaults.conf)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LWP-Model-Training\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Master: {spark.sparkContext.master}\")\n",
    "print(f\"Spark UI: http://localhost:4040\")\n",
    "print(\"\\nSpark Configuration:\")\n",
    "print(f\"  RAPIDS enabled: {spark.conf.get('spark.rapids.sql.enabled', 'false')}\")\n",
    "print(f\"  GPU resources: {spark.conf.get('spark.executor.resource.gpu.amount', 'none')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cargar Datos Históricos de StatsBomb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando competiciones...\n",
      "\n",
      "Temporadas disponibles de La Liga:\n",
      "    season_id season_name\n",
      "38         90   2020/2021\n",
      "39         42   2019/2020\n",
      "40          4   2018/2019\n",
      "41          1   2017/2018\n",
      "42          2   2016/2017\n",
      "43         27   2015/2016\n",
      "44         26   2014/2015\n",
      "45         25   2013/2014\n",
      "46         24   2012/2013\n",
      "47         23   2011/2012\n",
      "48         22   2010/2011\n",
      "49         21   2009/2010\n",
      "50         41   2008/2009\n",
      "51         40   2007/2008\n",
      "52         39   2006/2007\n",
      "53         38   2005/2006\n",
      "54         37   2004/2005\n",
      "55        278   1973/1974\n",
      "\n",
      "Temporadas seleccionadas: [90, 42, 4, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# La Liga competition ID\n",
    "COMPETITION_ID = 11  # La Liga\n",
    "\n",
    "print(\"Cargando competiciones...\")\n",
    "competitions = sb.competitions()\n",
    "la_liga = competitions[competitions['competition_id'] == COMPETITION_ID]\n",
    "print(f\"\\nTemporadas disponibles de La Liga:\")\n",
    "print(la_liga[['season_id', 'season_name']])\n",
    "\n",
    "# Get the last 4 seasons\n",
    "season_ids = [90,42,4,1,2]\n",
    "print(f\"\\nTemporadas seleccionadas: {season_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cargando partidos de temporada 90...\n",
      "  - 35 partidos cargados\n",
      "\n",
      "Cargando partidos de temporada 42...\n",
      "  - 33 partidos cargados\n",
      "\n",
      "Cargando partidos de temporada 4...\n",
      "  - 34 partidos cargados\n",
      "\n",
      "Cargando partidos de temporada 1...\n",
      "  - 36 partidos cargados\n",
      "\n",
      "Cargando partidos de temporada 2...\n",
      "  - 34 partidos cargados\n",
      "\n",
      "Total de partidos: 172\n",
      "Columnas: ['match_id', 'match_date', 'kick_off', 'competition', 'season', 'home_team', 'away_team', 'home_score', 'away_score', 'match_status', 'match_status_360', 'last_updated', 'last_updated_360', 'match_week', 'competition_stage', 'stadium', 'referee', 'home_managers', 'away_managers', 'data_version', 'shot_fidelity_version', 'xy_fidelity_version']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_id</th>\n",
       "      <th>match_date</th>\n",
       "      <th>kick_off</th>\n",
       "      <th>competition</th>\n",
       "      <th>season</th>\n",
       "      <th>home_team</th>\n",
       "      <th>away_team</th>\n",
       "      <th>home_score</th>\n",
       "      <th>away_score</th>\n",
       "      <th>match_status</th>\n",
       "      <th>...</th>\n",
       "      <th>last_updated_360</th>\n",
       "      <th>match_week</th>\n",
       "      <th>competition_stage</th>\n",
       "      <th>stadium</th>\n",
       "      <th>referee</th>\n",
       "      <th>home_managers</th>\n",
       "      <th>away_managers</th>\n",
       "      <th>data_version</th>\n",
       "      <th>shot_fidelity_version</th>\n",
       "      <th>xy_fidelity_version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3773386</td>\n",
       "      <td>2020-10-31</td>\n",
       "      <td>21:00:00.000</td>\n",
       "      <td>Spain - La Liga</td>\n",
       "      <td>2020/2021</td>\n",
       "      <td>Deportivo Alavés</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>available</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-07-25T04:25:41.348202</td>\n",
       "      <td>8</td>\n",
       "      <td>Regular Season</td>\n",
       "      <td>Estadio de Mendizorroza</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pablo Javier Machín Díez</td>\n",
       "      <td>Ronald Koeman</td>\n",
       "      <td>1.1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3773565</td>\n",
       "      <td>2021-01-09</td>\n",
       "      <td>18:30:00.000</td>\n",
       "      <td>Spain - La Liga</td>\n",
       "      <td>2020/2021</td>\n",
       "      <td>Granada</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>available</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-07-25T04:30:16.058384</td>\n",
       "      <td>18</td>\n",
       "      <td>Regular Season</td>\n",
       "      <td>Estadio Nuevo Los Cármenes</td>\n",
       "      <td>Ricardo De Burgos Bengoetxea</td>\n",
       "      <td>Diego Martínez Penas</td>\n",
       "      <td>Ronald Koeman</td>\n",
       "      <td>1.1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3773457</td>\n",
       "      <td>2021-05-16</td>\n",
       "      <td>18:30:00.000</td>\n",
       "      <td>Spain - La Liga</td>\n",
       "      <td>2020/2021</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Celta Vigo</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>available</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-04-27T23:03:53.506485</td>\n",
       "      <td>37</td>\n",
       "      <td>Regular Season</td>\n",
       "      <td>Spotify Camp Nou</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ronald Koeman</td>\n",
       "      <td>Eduardo Germán Coudet</td>\n",
       "      <td>1.1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3773631</td>\n",
       "      <td>2021-02-07</td>\n",
       "      <td>21:00:00.000</td>\n",
       "      <td>Spain - La Liga</td>\n",
       "      <td>2020/2021</td>\n",
       "      <td>Real Betis</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>available</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-07-25T03:56:34.733180</td>\n",
       "      <td>22</td>\n",
       "      <td>Regular Season</td>\n",
       "      <td>Estadio Benito Villamarín</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manuel Luis Pellegrini Ripamonti</td>\n",
       "      <td>Ronald Koeman</td>\n",
       "      <td>1.1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3773665</td>\n",
       "      <td>2021-03-06</td>\n",
       "      <td>21:00:00.000</td>\n",
       "      <td>Spain - La Liga</td>\n",
       "      <td>2020/2021</td>\n",
       "      <td>Osasuna</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>available</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-04-28T02:57:03.412841</td>\n",
       "      <td>26</td>\n",
       "      <td>Regular Season</td>\n",
       "      <td>Estadio El Sadar</td>\n",
       "      <td>Guillermo Cuadra Fernández</td>\n",
       "      <td>Jagoba Arrasate Elustondo</td>\n",
       "      <td>Ronald Koeman</td>\n",
       "      <td>1.1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_id  match_date      kick_off      competition     season  \\\n",
       "0   3773386  2020-10-31  21:00:00.000  Spain - La Liga  2020/2021   \n",
       "1   3773565  2021-01-09  18:30:00.000  Spain - La Liga  2020/2021   \n",
       "2   3773457  2021-05-16  18:30:00.000  Spain - La Liga  2020/2021   \n",
       "3   3773631  2021-02-07  21:00:00.000  Spain - La Liga  2020/2021   \n",
       "4   3773665  2021-03-06  21:00:00.000  Spain - La Liga  2020/2021   \n",
       "\n",
       "          home_team   away_team  home_score  away_score match_status  ...  \\\n",
       "0  Deportivo Alavés   Barcelona           1           1    available  ...   \n",
       "1           Granada   Barcelona           0           4    available  ...   \n",
       "2         Barcelona  Celta Vigo           1           2    available  ...   \n",
       "3        Real Betis   Barcelona           2           3    available  ...   \n",
       "4           Osasuna   Barcelona           0           2    available  ...   \n",
       "\n",
       "             last_updated_360 match_week competition_stage  \\\n",
       "0  2023-07-25T04:25:41.348202          8    Regular Season   \n",
       "1  2023-07-25T04:30:16.058384         18    Regular Season   \n",
       "2  2023-04-27T23:03:53.506485         37    Regular Season   \n",
       "3  2023-07-25T03:56:34.733180         22    Regular Season   \n",
       "4  2023-04-28T02:57:03.412841         26    Regular Season   \n",
       "\n",
       "                      stadium                       referee  \\\n",
       "0     Estadio de Mendizorroza                           NaN   \n",
       "1  Estadio Nuevo Los Cármenes  Ricardo De Burgos Bengoetxea   \n",
       "2            Spotify Camp Nou                           NaN   \n",
       "3   Estadio Benito Villamarín                           NaN   \n",
       "4            Estadio El Sadar    Guillermo Cuadra Fernández   \n",
       "\n",
       "                      home_managers          away_managers data_version  \\\n",
       "0          Pablo Javier Machín Díez          Ronald Koeman        1.1.0   \n",
       "1              Diego Martínez Penas          Ronald Koeman        1.1.0   \n",
       "2                     Ronald Koeman  Eduardo Germán Coudet        1.1.0   \n",
       "3  Manuel Luis Pellegrini Ripamonti          Ronald Koeman        1.1.0   \n",
       "4         Jagoba Arrasate Elustondo          Ronald Koeman        1.1.0   \n",
       "\n",
       "  shot_fidelity_version xy_fidelity_version  \n",
       "0                     2                   2  \n",
       "1                     2                   2  \n",
       "2                     2                   2  \n",
       "3                     2                   2  \n",
       "4                     2                   2  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all matches from the selected seasons\n",
    "all_matches = []\n",
    "\n",
    "for season_id in season_ids:\n",
    "    print(f\"\\nCargando partidos de temporada {season_id}...\")\n",
    "    matches = sb.matches(competition_id=COMPETITION_ID, season_id=season_id)\n",
    "    all_matches.append(matches)\n",
    "    print(f\"  - {len(matches)} partidos cargados\")\n",
    "\n",
    "matches_df = pd.concat(all_matches, ignore_index=True)\n",
    "print(f\"\\nTotal de partidos: {len(matches_df)}\")\n",
    "print(f\"Columnas: {matches_df.columns.tolist()}\")\n",
    "matches_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering - Extracción de Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_events(match_id, events_df):\n",
    "    \"\"\"\n",
    "    Extrae características para cada snapshot temporal del partido.\n",
    "    Cada fila representa el estado del partido en un momento dado.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Get match info\n",
    "    match_info = matches_df[matches_df['match_id'] == match_id].iloc[0]\n",
    "    home_team = match_info['home_team']\n",
    "    away_team = match_info['away_team']\n",
    "    home_score = match_info['home_score']\n",
    "    away_score = match_info['away_score']\n",
    "    \n",
    "    # Determine final result\n",
    "    if home_score > away_score:\n",
    "        result = 'home_win'\n",
    "    elif home_score < away_score:\n",
    "        result = 'away_win'\n",
    "    else:\n",
    "        result = 'draw'\n",
    "    \n",
    "    # Sort events by time\n",
    "    events_df = events_df.sort_values(['period', 'minute', 'second'])\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    current_home_score = 0\n",
    "    current_away_score = 0\n",
    "    home_shots = 0\n",
    "    away_shots = 0\n",
    "    home_passes = 0\n",
    "    away_passes = 0\n",
    "    \n",
    "    # Create snapshots every 5 minutes\n",
    "    for minute in range(0, 95, 5):\n",
    "        events_until_now = events_df[events_df['minute'] <= minute]\n",
    "        \n",
    "        if len(events_until_now) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Count events by team\n",
    "        home_events = events_until_now[events_until_now['team'] == home_team]\n",
    "        away_events = events_until_now[events_until_now['team'] == away_team]\n",
    "        \n",
    "        # Calculate current score\n",
    "        goals = events_until_now[events_until_now['type'] == 'Shot']\n",
    "        current_home_score = len(goals[(goals['team'] == home_team) & (goals['shot_outcome'] == 'Goal')])\n",
    "        current_away_score = len(goals[(goals['team'] == away_team) & (goals['shot_outcome'] == 'Goal')])\n",
    "        \n",
    "        # Calculate stats\n",
    "        home_shots = len(home_events[home_events['type'] == 'Shot'])\n",
    "        away_shots = len(away_events[away_events['type'] == 'Shot'])\n",
    "        home_passes = len(home_events[home_events['type'] == 'Pass'])\n",
    "        away_passes = len(away_events[away_events['type'] == 'Pass'])\n",
    "        \n",
    "        # Calculate possession (simplified)\n",
    "        total_events = len(home_events) + len(away_events)\n",
    "        home_possession = len(home_events) / total_events if total_events > 0 else 0.5\n",
    "        \n",
    "        # Create feature row\n",
    "        feature_row = {\n",
    "            'match_id': match_id,\n",
    "            'minute': minute,\n",
    "            'home_score': current_home_score,\n",
    "            'away_score': current_away_score,\n",
    "            'score_diff': current_home_score - current_away_score,\n",
    "            'home_shots': home_shots,\n",
    "            'away_shots': away_shots,\n",
    "            'shots_diff': home_shots - away_shots,\n",
    "            'home_passes': home_passes,\n",
    "            'away_passes': away_passes,\n",
    "            'passes_diff': home_passes - away_passes,\n",
    "            'home_possession': home_possession,\n",
    "            'time_remaining': 90 - minute,\n",
    "            'result': result\n",
    "        }\n",
    "        \n",
    "        features.append(feature_row)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo características de los partidos...\n",
      "Nota: Este proceso puede tomar varios minutos debido a las llamadas a la API.\n",
      "\n",
      "[1/50] Procesando match 3773386... ✓ 19 snapshots\n",
      "[2/50] Procesando match 3773565... ✓ 19 snapshots\n",
      "[3/50] Procesando match 3773457... ✓ 19 snapshots\n",
      "[4/50] Procesando match 3773631... ✓ 19 snapshots\n",
      "[5/50] Procesando match 3773665... ✓ 19 snapshots\n",
      "[6/50] Procesando match 3773497... ✓ 19 snapshots\n",
      "[7/50] Procesando match 3773660... ✓ 19 snapshots\n",
      "[8/50] Procesando match 3773593... ✓ 19 snapshots\n",
      "[9/50] Procesando match 3773466... ✓ 19 snapshots\n",
      "[10/50] Procesando match 3773585... ✓ 19 snapshots\n",
      "[11/50] Procesando match 3773552... ✓ 19 snapshots\n",
      "[12/50] Procesando match 3773672... ✓ 19 snapshots\n",
      "[13/50] Procesando match 3773587... ✓ 19 snapshots\n",
      "[14/50] Procesando match 3773656... ✓ 19 snapshots\n",
      "[15/50] Procesando match 3773377... ✓ 19 snapshots\n",
      "[16/50] Procesando match 3773586... ✓ 19 snapshots\n",
      "[17/50] Procesando match 3773372... ✓ 19 snapshots\n",
      "[18/50] Procesando match 3773387... ✓ 19 snapshots\n",
      "[19/50] Procesando match 3773695... ✓ 19 snapshots\n",
      "[20/50] Procesando match 3773369... ✓ 19 snapshots\n",
      "[21/50] Procesando match 3773661... ✓ 19 snapshots\n",
      "[22/50] Procesando match 3773597... ✓ 19 snapshots\n",
      "[23/50] Procesando match 3773523... ✓ 19 snapshots\n",
      "[24/50] Procesando match 3773571... ✓ 19 snapshots\n",
      "[25/50] Procesando match 3773428... ✓ 19 snapshots\n",
      "[26/50] Procesando match 3764661... ✓ 19 snapshots\n",
      "[27/50] Procesando match 3773526... ✓ 19 snapshots\n",
      "[28/50] Procesando match 3773474... ✓ 19 snapshots\n",
      "[29/50] Procesando match 3773625... ✓ 19 snapshots\n",
      "[30/50] Procesando match 3773403... ✓ 19 snapshots\n",
      "[31/50] Procesando match 3773547... ✓ 19 snapshots\n",
      "[32/50] Procesando match 3773415... ✓ 19 snapshots\n",
      "[33/50] Procesando match 3764440... ✓ 19 snapshots\n",
      "[34/50] Procesando match 3773689... ✓ 19 snapshots\n",
      "[35/50] Procesando match 3773477... ✓ 19 snapshots\n",
      "[36/50] Procesando match 303731... ✓ 19 snapshots\n",
      "[37/50] Procesando match 303532... ✓ 19 snapshots\n",
      "[38/50] Procesando match 303516... ✓ 19 snapshots\n",
      "[39/50] Procesando match 303596... ✓ 19 snapshots\n",
      "[40/50] Procesando match 303430... ✓ 19 snapshots\n",
      "[41/50] Procesando match 303725... ✓ 19 snapshots\n",
      "[42/50] Procesando match 303504... ✓ 19 snapshots\n",
      "[43/50] Procesando match 303451... ✓ 19 snapshots\n",
      "[44/50] Procesando match 303664... ✓ 19 snapshots\n",
      "[45/50] Procesando match 303682... ✓ 19 snapshots\n",
      "[46/50] Procesando match 303400... ✓ 19 snapshots\n",
      "[47/50] Procesando match 303634... ✓ 19 snapshots\n",
      "[48/50] Procesando match 303421... ✓ 19 snapshots\n",
      "[49/50] Procesando match 303493... ✓ 19 snapshots\n",
      "[50/50] Procesando match 303680... ✓ 19 snapshots\n",
      "\n",
      "Total de snapshots (filas): 950\n",
      "Distribución de resultados:\n",
      "home_win    437\n",
      "away_win    342\n",
      "draw        171\n",
      "Name: result, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_id</th>\n",
       "      <th>minute</th>\n",
       "      <th>home_score</th>\n",
       "      <th>away_score</th>\n",
       "      <th>score_diff</th>\n",
       "      <th>home_shots</th>\n",
       "      <th>away_shots</th>\n",
       "      <th>shots_diff</th>\n",
       "      <th>home_passes</th>\n",
       "      <th>away_passes</th>\n",
       "      <th>passes_diff</th>\n",
       "      <th>home_possession</th>\n",
       "      <th>time_remaining</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3773386</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>-17</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>90</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3773386</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>61</td>\n",
       "      <td>-28</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>85</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3773386</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>93</td>\n",
       "      <td>-36</td>\n",
       "      <td>0.407921</td>\n",
       "      <td>80</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3773386</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>76</td>\n",
       "      <td>126</td>\n",
       "      <td>-50</td>\n",
       "      <td>0.397626</td>\n",
       "      <td>75</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3773386</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>92</td>\n",
       "      <td>178</td>\n",
       "      <td>-86</td>\n",
       "      <td>0.368192</td>\n",
       "      <td>70</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3773386</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "      <td>101</td>\n",
       "      <td>202</td>\n",
       "      <td>-101</td>\n",
       "      <td>0.366442</td>\n",
       "      <td>65</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3773386</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>114</td>\n",
       "      <td>254</td>\n",
       "      <td>-140</td>\n",
       "      <td>0.350740</td>\n",
       "      <td>60</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3773386</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-2</td>\n",
       "      <td>116</td>\n",
       "      <td>320</td>\n",
       "      <td>-204</td>\n",
       "      <td>0.319051</td>\n",
       "      <td>55</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3773386</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-2</td>\n",
       "      <td>129</td>\n",
       "      <td>353</td>\n",
       "      <td>-224</td>\n",
       "      <td>0.323565</td>\n",
       "      <td>50</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3773386</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>-3</td>\n",
       "      <td>135</td>\n",
       "      <td>389</td>\n",
       "      <td>-254</td>\n",
       "      <td>0.317884</td>\n",
       "      <td>45</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_id  minute  home_score  away_score  score_diff  home_shots  \\\n",
       "0   3773386       0           0           0           0           0   \n",
       "1   3773386       5           0           0           0           0   \n",
       "2   3773386      10           0           0           0           0   \n",
       "3   3773386      15           0           0           0           0   \n",
       "4   3773386      20           0           0           0           1   \n",
       "5   3773386      25           0           0           0           1   \n",
       "6   3773386      30           1           0           1           2   \n",
       "7   3773386      35           1           0           1           2   \n",
       "8   3773386      40           1           0           1           2   \n",
       "9   3773386      45           1           0           1           3   \n",
       "\n",
       "   away_shots  shots_diff  home_passes  away_passes  passes_diff  \\\n",
       "0           0           0            2           19          -17   \n",
       "1           0           0           33           61          -28   \n",
       "2           0           0           57           93          -36   \n",
       "3           1          -1           76          126          -50   \n",
       "4           2          -1           92          178          -86   \n",
       "5           3          -2          101          202         -101   \n",
       "6           3          -1          114          254         -140   \n",
       "7           4          -2          116          320         -204   \n",
       "8           4          -2          129          353         -224   \n",
       "9           6          -3          135          389         -254   \n",
       "\n",
       "   home_possession  time_remaining result  \n",
       "0         0.225352              90   draw  \n",
       "1         0.398148              85   draw  \n",
       "2         0.407921              80   draw  \n",
       "3         0.397626              75   draw  \n",
       "4         0.368192              70   draw  \n",
       "5         0.366442              65   draw  \n",
       "6         0.350740              60   draw  \n",
       "7         0.319051              55   draw  \n",
       "8         0.323565              50   draw  \n",
       "9         0.317884              45   draw  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract features from all matches\n",
    "print(\"Extrayendo características de los partidos...\")\n",
    "print(\"Nota: Este proceso puede tomar varios minutos debido a las llamadas a la API.\\n\")\n",
    "\n",
    "all_features = []\n",
    "match_ids = matches_df['match_id'].head(50).tolist()  # Use first 50 matches for training\n",
    "\n",
    "for i, match_id in enumerate(match_ids, 1):\n",
    "    try:\n",
    "        print(f\"[{i}/{len(match_ids)}] Procesando match {match_id}...\", end=\" \")\n",
    "        events = sb.events(match_id=match_id, split=False, flatten_attrs=True)\n",
    "        features = extract_features_from_events(match_id, events)\n",
    "        all_features.extend(features)\n",
    "        print(f\"✓ {len(features)} snapshots\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "features_pd = pd.DataFrame(all_features)\n",
    "print(f\"\\nTotal de snapshots (filas): {len(features_pd)}\")\n",
    "print(f\"Distribución de resultados:\")\n",
    "print(features_pd['result'].value_counts())\n",
    "features_pd.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preparar Datos para Entrenamiento en Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark DataFrame creado:\n",
      "root\n",
      " |-- match_id: long (nullable = true)\n",
      " |-- minute: long (nullable = true)\n",
      " |-- home_score: long (nullable = true)\n",
      " |-- away_score: long (nullable = true)\n",
      " |-- score_diff: long (nullable = true)\n",
      " |-- home_shots: long (nullable = true)\n",
      " |-- away_shots: long (nullable = true)\n",
      " |-- shots_diff: long (nullable = true)\n",
      " |-- home_passes: long (nullable = true)\n",
      " |-- away_passes: long (nullable = true)\n",
      " |-- passes_diff: long (nullable = true)\n",
      " |-- home_possession: double (nullable = true)\n",
      " |-- time_remaining: long (nullable = true)\n",
      " |-- result: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Número de filas: 950\n",
      "+--------+------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-------------------+--------------+------+\n",
      "|match_id|minute|home_score|away_score|score_diff|home_shots|away_shots|shots_diff|home_passes|away_passes|passes_diff|    home_possession|time_remaining|result|\n",
      "+--------+------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-------------------+--------------+------+\n",
      "| 3773386|     0|         0|         0|         0|         0|         0|         0|          2|         19|        -17|0.22535211267605634|            90|  draw|\n",
      "| 3773386|     5|         0|         0|         0|         0|         0|         0|         33|         61|        -28|0.39814814814814814|            85|  draw|\n",
      "| 3773386|    10|         0|         0|         0|         0|         0|         0|         57|         93|        -36| 0.4079207920792079|            80|  draw|\n",
      "| 3773386|    15|         0|         0|         0|         0|         1|        -1|         76|        126|        -50|0.39762611275964393|            75|  draw|\n",
      "| 3773386|    20|         0|         0|         0|         1|         2|        -1|         92|        178|        -86| 0.3681917211328976|            70|  draw|\n",
      "| 3773386|    25|         0|         0|         0|         1|         3|        -2|        101|        202|       -101|0.36644165863066536|            65|  draw|\n",
      "| 3773386|    30|         1|         0|         1|         2|         3|        -1|        114|        254|       -140|0.35074045206547155|            60|  draw|\n",
      "| 3773386|    35|         1|         0|         1|         2|         4|        -2|        116|        320|       -204| 0.3190507580751483|            55|  draw|\n",
      "| 3773386|    40|         1|         0|         1|         2|         4|        -2|        129|        353|       -224| 0.3235645933014354|            50|  draw|\n",
      "| 3773386|    45|         1|         0|         1|         3|         6|        -3|        135|        389|       -254| 0.3178844056706652|            45|  draw|\n",
      "+--------+------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-------------------+--------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(features_pd)\n",
    "\n",
    "print(\"Spark DataFrame creado:\")\n",
    "spark_df.printSchema()\n",
    "print(f\"\\nNúmero de filas: {spark_df.count()}\")\n",
    "spark_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos preparados para entrenamiento:\n",
      "+---------------------------------------------------------------------------+-----+------+\n",
      "|features                                                                   |label|result|\n",
      "+---------------------------------------------------------------------------+-----+------+\n",
      "|(12,[7,8,9,10,11],[2.0,19.0,-17.0,0.22535211267605634,90.0])               |2.0  |draw  |\n",
      "|(12,[0,7,8,9,10,11],[5.0,33.0,61.0,-28.0,0.39814814814814814,85.0])        |2.0  |draw  |\n",
      "|(12,[0,7,8,9,10,11],[10.0,57.0,93.0,-36.0,0.4079207920792079,80.0])        |2.0  |draw  |\n",
      "|[15.0,0.0,0.0,0.0,0.0,1.0,-1.0,76.0,126.0,-50.0,0.39762611275964393,75.0]  |2.0  |draw  |\n",
      "|[20.0,0.0,0.0,0.0,1.0,2.0,-1.0,92.0,178.0,-86.0,0.3681917211328976,70.0]   |2.0  |draw  |\n",
      "|[25.0,0.0,0.0,0.0,1.0,3.0,-2.0,101.0,202.0,-101.0,0.36644165863066536,65.0]|2.0  |draw  |\n",
      "|[30.0,1.0,0.0,1.0,2.0,3.0,-1.0,114.0,254.0,-140.0,0.35074045206547155,60.0]|2.0  |draw  |\n",
      "|[35.0,1.0,0.0,1.0,2.0,4.0,-2.0,116.0,320.0,-204.0,0.3190507580751483,55.0] |2.0  |draw  |\n",
      "|[40.0,1.0,0.0,1.0,2.0,4.0,-2.0,129.0,353.0,-224.0,0.3235645933014354,50.0] |2.0  |draw  |\n",
      "|[45.0,1.0,0.0,1.0,3.0,6.0,-3.0,135.0,389.0,-254.0,0.3178844056706652,45.0] |2.0  |draw  |\n",
      "+---------------------------------------------------------------------------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and labels\n",
    "feature_cols = [\n",
    "    'minute', 'home_score', 'away_score', 'score_diff',\n",
    "    'home_shots', 'away_shots', 'shots_diff',\n",
    "    'home_passes', 'away_passes', 'passes_diff',\n",
    "    'home_possession', 'time_remaining'\n",
    "]\n",
    "\n",
    "# Create vector assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Create label indexer (convert string labels to numeric)\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"result\",\n",
    "    outputCol=\"label\"\n",
    ")\n",
    "\n",
    "# Transform data\n",
    "data_with_features = assembler.transform(spark_df)\n",
    "data_with_labels = label_indexer.fit(data_with_features).transform(data_with_features)\n",
    "\n",
    "print(\"Datos preparados para entrenamiento:\")\n",
    "data_with_labels.select('features', 'label', 'result').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 763 filas\n",
      "Test set: 187 filas\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "train_data, test_data = data_with_labels.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training set: {train_data.count()} filas\")\n",
    "print(f\"Test set: {test_data.count()} filas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entrenar Modelo de Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo Random Forest con GPU...\n",
      "Revisa Spark UI en http://localhost:4040 para métricas de rendimiento\n",
      "\n",
      "25/11/06 04:44:21 WARN DAGScheduler: Broadcasting large task binary with size 1091.0 KiB\n",
      "25/11/06 04:44:22 WARN DAGScheduler: Broadcasting large task binary with size 1462.8 KiB\n",
      "25/11/06 04:44:22 WARN DAGScheduler: Broadcasting large task binary with size 1843.4 KiB\n",
      "25/11/06 04:44:23 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "✓ Modelo entrenado en 4.55 segundos\n",
      "✓ Número de árboles: 100\n",
      "✓ Feature importances disponibles\n"
     ]
    }
   ],
   "source": [
    "# Create Random Forest classifier\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Entrenando modelo Random Forest con GPU...\")\n",
    "print(\"Revisa Spark UI en http://localhost:4040 para métricas de rendimiento\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "rf_model = rf.fit(train_data)\n",
    "training_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "print(f\"✓ Modelo entrenado en {training_time:.2f} segundos\")\n",
    "print(f\"✓ Número de árboles: {rf_model.getNumTrees}\")\n",
    "print(f\"✓ Feature importances disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones del modelo:\n",
      "25/11/06 04:44:36 WARN DAGScheduler: Broadcasting large task binary with size 1652.6 KiB\n",
      "+------+----------+--------+-----+----------+--------------------------------------------------------------+\n",
      "|minute|score_diff|result  |label|prediction|probability                                                   |\n",
      "+------+----------+--------+-----+----------+--------------------------------------------------------------+\n",
      "|10    |0         |draw    |2.0  |1.0       |[0.2774039370570409,0.5583783577940511,0.164217705148908]     |\n",
      "|30    |1         |draw    |2.0  |1.0       |[0.3200557924228494,0.5270165103649969,0.1529276972121536]    |\n",
      "|40    |1         |draw    |2.0  |2.0       |[0.14277476802417793,0.3071699612147394,0.5500552707610826]   |\n",
      "|65    |0         |draw    |2.0  |2.0       |[0.02,0.18749969939605937,0.7925003006039407]                 |\n",
      "|0     |0         |away_win|1.0  |0.0       |[0.7583378396274043,0.162545551281854,0.07911660909074174]    |\n",
      "|20    |0         |away_win|1.0  |0.0       |[0.6225246733997161,0.3104657346847056,0.06700959191557822]   |\n",
      "|50    |0         |away_win|1.0  |1.0       |[0.10030798279273329,0.8224087188893864,0.07728329831788029]  |\n",
      "|80    |0         |away_win|1.0  |1.0       |[0.023431034378860467,0.884516164994426,0.09205280062671367]  |\n",
      "|35    |2         |home_win|0.0  |0.0       |[0.9513333333333334,0.018666666666666665,0.03]                |\n",
      "|40    |2         |home_win|0.0  |0.0       |[0.95,0.02,0.03]                                              |\n",
      "|45    |2         |home_win|0.0  |0.0       |[0.99,0.01,0.0]                                               |\n",
      "|55    |2         |home_win|0.0  |0.0       |[0.98,0.02,0.0]                                               |\n",
      "|65    |1         |home_win|0.0  |0.0       |[0.907267003841467,0.06058841872458177,0.032144577433951244]  |\n",
      "|85    |1         |home_win|0.0  |0.0       |[0.8511625157629257,0.13845286885245903,0.010384615384615386] |\n",
      "|25    |-1        |away_win|1.0  |1.0       |[0.04664765134078717,0.8342085356809588,0.11914381297825401]  |\n",
      "|60    |-3        |away_win|1.0  |1.0       |[0.011034688223115334,0.987541001400555,0.0014243103763296866]|\n",
      "|5     |0         |away_win|1.0  |1.0       |[0.2741729793361492,0.5373891839619285,0.1884378367019223]    |\n",
      "|5     |0         |away_win|1.0  |1.0       |[0.22476282478976856,0.6842373887908538,0.09099978641937755]  |\n",
      "|65    |-1        |away_win|1.0  |1.0       |[0.00311483488852307,0.9948684856140061,0.002016679497470955] |\n",
      "|85    |-2        |away_win|1.0  |1.0       |[0.0015073418913241895,0.9764759786112048,0.02201667949747095]|\n",
      "+------+----------+--------+-----+----------+--------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test set\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "print(\"Predicciones del modelo:\")\n",
    "predictions.select('minute', 'score_diff', 'result', 'label', 'prediction', 'probability').show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/06 04:44:54 WARN DAGScheduler: Broadcasting large task binary with size 1666.0 KiB\n",
      "25/11/06 04:44:54 WARN DAGScheduler: Broadcasting large task binary with size 1666.0 KiB\n",
      "==================================================\n",
      "MÉTRICAS DEL MODELO\n",
      "==================================================\n",
      "Accuracy: 0.8396\n",
      "F1 Score: 0.8382\n",
      "Training Time: 4.55 seconds\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "f1_score = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"MÉTRICAS DEL MODELO\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importancia de características:\n",
      "            feature  importance\n",
      "3        score_diff    0.190174\n",
      "10  home_possession    0.157861\n",
      "6        shots_diff    0.108214\n",
      "9       passes_diff    0.098928\n",
      "5        away_shots    0.081349\n",
      "4        home_shots    0.066008\n",
      "2        away_score    0.065497\n",
      "1        home_score    0.061120\n",
      "8       away_passes    0.056377\n",
      "7       home_passes    0.049368\n",
      "0            minute    0.041527\n",
      "11   time_remaining    0.023578\n"
     ]
    }
   ],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.featureImportances.toArray()\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nImportancia de características:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Guardar Modelo Entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando modelo en /work/models/lwp_model...\n",
      "✓ Modelo guardado exitosamente\n",
      "\n",
      "Mapeo de etiquetas: ['home_win', 'away_win', 'draw']\n",
      "  0 = Victoria Local (home_win)\n",
      "  1 = Empate (draw)\n",
      "  2 = Victoria Visitante (away_win)\n"
     ]
    }
   ],
   "source": [
    "# Save model to disk\n",
    "MODEL_PATH = \"/work/models/lwp_model\"\n",
    "\n",
    "print(f\"Guardando modelo en {MODEL_PATH}...\")\n",
    "rf_model.write().overwrite().save(MODEL_PATH)\n",
    "print(\"✓ Modelo guardado exitosamente\")\n",
    "\n",
    "# Save label mapping\n",
    "label_mapping = label_indexer.fit(data_with_features).labels\n",
    "print(f\"\\nMapeo de etiquetas: {label_mapping}\")\n",
    "print(\"  0 = Victoria Local (home_win)\")\n",
    "print(\"  1 = Empate (draw)\")\n",
    "print(\"  2 = Victoria Visitante (away_win)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test de Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test de inferencia con escenarios de ejemplo:\n",
      "25/11/06 04:45:16 WARN DAGScheduler: Broadcasting large task binary with size 1624.0 KiB\n",
      "25/11/06 04:45:16 WARN DAGScheduler: Broadcasting large task binary with size 1624.0 KiB\n",
      "25/11/06 04:45:16 WARN DAGScheduler: Broadcasting large task binary with size 1624.0 KiB\n",
      "+------+----------+---------------+--------------+----------+------------------------------------------------------------+\n",
      "|minute|score_diff|home_possession|time_remaining|prediction|probability                                                 |\n",
      "+------+----------+---------------+--------------+----------+------------------------------------------------------------+\n",
      "|70    |2         |0.58           |20            |0.0       |[0.97,0.0,0.03]                                             |\n",
      "|45    |0         |0.5            |45            |1.0       |[0.08806183902755549,0.4833456566543206,0.428592504318124]  |\n",
      "|80    |-1        |0.56           |10            |1.0       |[0.03736078431372549,0.6066889951725372,0.35595022051373737]|\n",
      "+------+----------+---------------+--------------+----------+------------------------------------------------------------+\n",
      "\n",
      "\n",
      "Interpretación de probabilidades:\n",
      "probability[0] = P(Victoria Local)\n",
      "probability[1] = P(Empate)\n",
      "probability[2] = P(Victoria Visitante)\n"
     ]
    }
   ],
   "source": [
    "# Test inference with sample data\n",
    "test_scenario = spark.createDataFrame([\n",
    "    # Scenario 1: Home team winning 2-0 at minute 70\n",
    "    (70, 2, 0, 2, 8, 3, 5, 250, 180, 70, 0.58, 20),\n",
    "    # Scenario 2: Tied 1-1 at minute 45\n",
    "    (45, 1, 1, 0, 5, 5, 0, 200, 200, 0, 0.50, 45),\n",
    "    # Scenario 3: Away team leading 0-1 at minute 80\n",
    "    (80, 0, 1, -1, 6, 8, -2, 280, 220, -60, 0.56, 10),\n",
    "], feature_cols)\n",
    "\n",
    "test_features = assembler.transform(test_scenario)\n",
    "test_predictions = rf_model.transform(test_features)\n",
    "\n",
    "print(\"Test de inferencia con escenarios de ejemplo:\")\n",
    "test_predictions.select(\n",
    "    'minute', 'score_diff', 'home_possession', 'time_remaining',\n",
    "    'prediction', 'probability'\n",
    ").show(truncate=False)\n",
    "\n",
    "print(\"\\nInterpretación de probabilidades:\")\n",
    "print(\"probability[0] = P(Victoria Local)\")\n",
    "print(\"probability[1] = P(Empate)\")\n",
    "print(\"probability[2] = P(Victoria Visitante)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resumen y Próximos Pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RESUMEN DEL ENTRENAMIENTO\n",
      "============================================================\n",
      "✓ Modelo: Random Forest Classifier\n",
      "✓ Datos: 172 partidos procesados\n",
      "✓ Features: 12 características\n",
      "✓ Training samples: 763\n",
      "✓ Test samples: 187\n",
      "✓ Accuracy: 0.8396\n",
      "✓ F1 Score: 0.8382\n",
      "✓ Training time: 4.55 seconds\n",
      "✓ Modelo guardado en: /work/models/lwp_model\n",
      "============================================================\n",
      "\n",
      "PRÓXIMOS PASOS:\n",
      "1. Ejecutar notebook 03_Streaming_Estadisticas.ipynb\n",
      "2. Ejecutar notebook 04_Streaming_Inferencia_LWP.ipynb\n",
      "3. Capturar métricas desde Spark UI (localhost:4040)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"RESUMEN DEL ENTRENAMIENTO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✓ Modelo: Random Forest Classifier\")\n",
    "print(f\"✓ Datos: {len(matches_df)} partidos procesados\")\n",
    "print(f\"✓ Features: {len(feature_cols)} características\")\n",
    "print(f\"✓ Training samples: {train_data.count()}\")\n",
    "print(f\"✓ Test samples: {test_data.count()}\")\n",
    "print(f\"✓ Accuracy: {accuracy:.4f}\")\n",
    "print(f\"✓ F1 Score: {f1_score:.4f}\")\n",
    "print(f\"✓ Training time: {training_time:.2f} seconds\")\n",
    "print(f\"✓ Modelo guardado en: {MODEL_PATH}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nPRÓXIMOS PASOS:\")\n",
    "print(\"1. Ejecutar notebook 03_Streaming_Estadisticas.ipynb\")\n",
    "print(\"2. Ejecutar notebook 04_Streaming_Inferencia_LWP.ipynb\")\n",
    "print(\"3. Capturar métricas desde Spark UI (localhost:4040)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nota: Spark session sigue activa para exploración adicional.\n",
      "Ejecuta 'spark.stop()' cuando termines.\n",
      "25/11/06 04:46:34 ERROR TaskSchedulerImpl: Lost executor 0 on 172.19.0.6: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "25/11/06 04:46:34 ERROR Utils: Uncaught exception in thread dispatcher-CoarseGrainedScheduler\n",
      "org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:178)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:150)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.send(NettyRpcEnv.scala:193)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.send(NettyRpcEnv.scala:563)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$reviveOffers$1(CoarseGrainedSchedulerBackend.scala:630)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.reviveOffers(CoarseGrainedSchedulerBackend.scala:630)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.executorLost(TaskSchedulerImpl.scala:1004)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$removeExecutor(CoarseGrainedSchedulerBackend.scala:434)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.$anonfun$onDisconnected$1(CoarseGrainedSchedulerBackend.scala:345)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.$anonfun$onDisconnected$1$adapted(CoarseGrainedSchedulerBackend.scala:344)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.onDisconnected(CoarseGrainedSchedulerBackend.scala:344)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/11/06 04:46:34 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-f8cb7916-6d35-4b54-a6bb-39a3b63950a7/userFiles-1669f92b-f9e4-4b19-b340-860a76724c9d. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/spark-f8cb7916-6d35-4b54-a6bb-39a3b63950a7/userFiles-1669f92b-f9e4-4b19-b340-860a76724c9d\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1206)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2140)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2140)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:660)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 43958)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Stop Spark session\n",
    "# spark.stop()\n",
    "print(\"\\nNota: Spark session sigue activa para exploración adicional.\")\n",
    "print(\"Ejecuta 'spark.stop()' cuando termines.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
